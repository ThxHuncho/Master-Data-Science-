{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TP 2 : RL\n",
        "\n",
        "Le but de ce TP sera de découvrir et mettre en place un algorithme d'apprentissage par renforcement profond : le DQN.\n",
        "Vous apprendrez à un agent à jouer au jeu du cartpole, qui consiste à déplacer une plateforme sur laquelle est posé un baton de manière à ce que le baton reste en équilibre sur la plateforme.\n",
        "\n",
        "Nous allons utiliser la librairie TensorFlow et faire tourner notre code sous Google Colab pour nos TP, ce qui aura l'avantage de nous permettre d'utiliser les GPUs mis à disposition gratuitement par Google.\n",
        "Vous êtes libre d'utiliser une autre librairie que TensorFlow si vous en maîtrisez une autre ou votre machine personnelle si celle-ci possède un GPU suffisant, mais il est probable que les phases d'entraînement soit plus rapides sous Colab.\n",
        "\n",
        "N'hésitez pas à vous référer aux docs de TensorFlow 2 et Keras disponibles sur internet lors de ce TP.\n",
        "\n",
        "Ce TP peut s'effectue individuellement.\n",
        "Veuillez respecter les consignes suivantes pour le rendu de votre travail :\n",
        "\n",
        "*   Renommez le selon le format suivant : \"TP_RL_prenom_nom.ipynb\".\n",
        "*   Veillez à ce que votre nom et prénom soient complétés dans la cellule ci-dessous.\n",
        "*   Veillez à avoir bien exécuté toutes les cellules de code et que les résultats soient tous bien visible dans le notebook sans nécessiter une ré-exécution.\n",
        "*   Partagez le notebook avec ranvier.thomas.pro@gmail.com.\n",
        "\n",
        "Si vous avez effectué le TP autrement que sur Google Colab :\n",
        "\n",
        "*   Renommez le selon le format suivant : \"TP_RL_prenom_nom.ipynb\".\n",
        "*   Téléchargez le fichier ipynb.\n",
        "*   Envoyez le fichier en pièce-jointe à ranvier.thomas.pro@gmail.com, en indiquant en tant qu'objet : \"TP RL prenom nom\".\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NAo7olMnQer5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Veuillez compléter vos noms et prénoms ci-dessous :\n",
        "\n",
        "*   **Prenom** : ...\n",
        "*   **Nom** : ..."
      ],
      "metadata": {
        "id": "F9j4xHTgRV94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym2jEQKfK94R",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Installation des dépendances\n",
        "\n",
        "%%capture\n",
        "!pip install gym pyvirtualdisplay\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "!pip install colabgymrender==1.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9weXyrTUH7Ii"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import numpy as np\n",
        "from colabgymrender.recorder import Recorder\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Visualisation d'un épisode de Cartpole\n",
        "\n",
        "Vous allez utiliser l'environement cartpole proposé par la librairie gym. A chaque étape l'environement nous envoie une observation correspondant à l'état actuel de l'environement et on doit envoyer une action (0 ou 1 pour gauche ou droite) en réponse.\n",
        "\n",
        "Commencez par visualiser un épisode en ne sélectionnant que des actions aléatoires."
      ],
      "metadata": {
        "id": "DKa9tv9DRnAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment initialization\n",
        "env = gym.make('CartPole-v1')\n",
        "# Put the environment in a wrapper to save the episode video\n",
        "env = Recorder(env, './example')\n",
        "\n",
        "# Start a new episode and get the first environment state\n",
        "state = env.reset()\n",
        "done = False\n",
        "# While the episode is not over\n",
        "while not done:\n",
        "    # Select a random action\n",
        "    action_index = random.randrange(env.action_space.n)\n",
        "    # Execute selected action in the environment and get new observation and reward\n",
        "    next_state, reward, done, _ = env.step(action_index)\n",
        "\n",
        "# Play the episode video\n",
        "env.play()"
      ],
      "metadata": {
        "id": "fdxFSqSwRpxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Déclaration des hyperparamètres\n",
        "\n",
        "Le but de ce TP n'étant pas de chercher les meilleurs hyperparamètres possibles vous trouverez ci-dessous l'initilisation d'hyperparamètres par défaut qui donnent de bons résultats."
      ],
      "metadata": {
        "id": "DIgNg6VwamUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Get environment observation size and number of possible actions\n",
        "obs_size = env.observation_space.shape[0]\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "#Hyper Parameters\n",
        "training_episodes = 200                     # Max number of training episodes\n",
        "gamma = .95                                 # Discount factor used in Q-function\n",
        "learning_rate = .001                        # Learning rate of the optimizer\n",
        "batch_size = 24\n",
        "epsilon_max = 1.                            # Maximum epsilon value at 1.\n",
        "epsilon_min = .05                           # Minimum epsilon value at .05\n",
        "epsilon_decay = .995                        # Epsilon decay factor at .995\n",
        "replay_memory_capacity = 2500               # Limit memory capacity at 2500\n",
        "print_step = 10                             # Print recap every 10 steps\n",
        "target_avg_reward = 200                     # Early stop if avg reward >= 200"
      ],
      "metadata": {
        "id": "p_BEbYTGECeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Création de la replay memory\n",
        "\n",
        "La replay memory est utilisée pour stocker les transitions entre l'agent et l'environement. Chaque transition stockée est composée de l'état de départ, l'action réalisée par l'agent, l'état d'arrivée, le reward obtenu et si l'état d'arrivée est terminal ou non."
      ],
      "metadata": {
        "id": "Cqe9HTaSa90g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self._capacity = capacity\n",
        "        self._state_h = []\n",
        "        self._next_state_h = []\n",
        "        self._action_h = []\n",
        "        self._reward_h = []\n",
        "        self._done_h = []\n",
        "\n",
        "    def push(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Cette méthode doit permettre de stocker une transition dans la mémoire\n",
        "        tout en s'assurant que le nombre d'transitions stockée ne dépasse pas\n",
        "        sa capacité.\n",
        "        Si c'est le cas la nouvelle transition doit écraser la plus ancienne.\n",
        "\n",
        "        Une transition contient :\n",
        "            - state: np.array, l'état de départ\n",
        "            - next_state: np.array, l'état d'arrivée\n",
        "            - action: tf.tensor, l'action effectuée. Dans un soucis d'efficience\n",
        "                      vous devez stocker l'action directement sous forme de\n",
        "                      one-hot tensor, obtenu avec : action = tf.one_hot(action_index, nb_actions)\n",
        "            - reward: int, le reward obtenu grâce à l'action effectuée\n",
        "            - done: boolean, indique si le baton est tombé ou non\n",
        "        \"\"\"\n",
        "        self._state_h.append(state)\n",
        "        self._next_state_h.append(next_state)\n",
        "        self._action_h.append(action)\n",
        "        self._reward_h.append(reward)\n",
        "        self._done_h.append(done)\n",
        "        if len(self._state_h) >= self._capacity:\n",
        "            del self._state_h[0]\n",
        "            del self._next_state_h[0]\n",
        "            del self._action_h[0]\n",
        "            del self._reward_h[0]\n",
        "            del self._done_h[0]\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Cette méthode doit extraire une minibatch de la taille voulue de la\n",
        "        mémoire de manière aléatoire. Elle prend en entrée la taille de la batch\n",
        "        à extraire.\n",
        "\n",
        "        La méthode doit retourner :\n",
        "            - state_s: np.array, contenant les états de départ\n",
        "            - state_next_s: np.array, contenant les états d'arrivées\n",
        "            - action_s: tf.tensor, contenant les one-hots vectors des actions\n",
        "            - rewards_s: tf.tensor, contenant rewards\n",
        "            - opp_done_s: tf.tensor, contenant l'opposé du flag 'done' converti en float\n",
        "        \"\"\"\n",
        "        indices = np.random.choice(range(len(self._state_h)), size=batch_size)\n",
        "\n",
        "        state_s = np.array([self._state_h[i] for i in indices])\n",
        "        state_next_s = np.array([self._next_state_h[i] for i in indices])\n",
        "        action_s = tf.convert_to_tensor([self._action_h[i] for i in indices])\n",
        "        rewards_s = tf.convert_to_tensor([self._reward_h[i] for i in indices])\n",
        "        opp_done_s = tf.convert_to_tensor([1. - float(self._done_h[i]) for i in indices])\n",
        "\n",
        "        return state_s, state_next_s, action_s, rewards_s, opp_done_s\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Cette méthode override les appels de length sur l'objet ReplayMemory.\n",
        "        Elle doit retourner la quantité actuelle de transitions sauvegardées\n",
        "        dans la mémoire.\n",
        "        \"\"\"\n",
        "        return len(self._state_h)"
      ],
      "metadata": {
        "id": "J7PFieKDEHwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Création de l'agent\n",
        "\n",
        "L'agent apprend à intéragir et executer ses actions avec l'environnement."
      ],
      "metadata": {
        "id": "0hgJHikO_k-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, replay_memory, model, optimizer, loss_function, env):\n",
        "        self._rm = replay_memory\n",
        "        self._epsilon = epsilon_max\n",
        "        self._model = model\n",
        "        self._optimizer = optimizer\n",
        "        self._loss_function = loss_function\n",
        "        self._env = env\n",
        "        # History\n",
        "        self._reward_hist = []\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Méthode principale, lance l'entraînement de l'agent.\n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "        avg_reward = 0\n",
        "        for episode in range(training_episodes):\n",
        "            # Run 1 episode\n",
        "            episode_reward = self._run_episode()\n",
        "            self._reward_hist.append(episode_reward)\n",
        "            avg_reward += episode_reward\n",
        "            if (episode + 1) % print_step == 0:\n",
        "                print(f'episode: {episode + 1}/{training_episodes}, avg reward: {avg_reward / print_step}, e: {self._epsilon}')\n",
        "                # Early stop\n",
        "                if avg_reward / print_step >= target_avg_reward:\n",
        "                    break\n",
        "                avg_reward = 0\n",
        "        print(f'training done in {time.time() - start:.2f}s')\n",
        "        self._plot_history()\n",
        "\n",
        "    def _run_episode(self):\n",
        "        \"\"\"\n",
        "        Cette méthode doit executer un épisode d'apprentissage.\n",
        "        \"\"\"\n",
        "        # Initialize environment and get first state\n",
        "        # convert the state to np.array type\n",
        "        ...\n",
        "\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Select an action, randomly in epsilon % of the time, and using\n",
        "            # the model output otherwise.\n",
        "            # Obtain the model output: self._model(np.expand_dims(state, axis=0))\n",
        "            # You need to convert the model output which is a score for each\n",
        "            # possible action to the index of the action for which the model\n",
        "            # predicts the highest score.\n",
        "            ...\n",
        "\n",
        "            # Perform selected action in the environment, get next state, reward\n",
        "            # and done status\n",
        "            ...\n",
        "\n",
        "            # Convert the next state to np.array type\n",
        "            ...\n",
        "\n",
        "            episode_reward += reward\n",
        "            # Convert the selected action index to a one-hot tensor for\n",
        "            # efficiency (cf. tf.one_hot())\n",
        "            ...\n",
        "\n",
        "            # Add observation to the replay memory (cf. self._rm.push)\n",
        "            ...\n",
        "            \n",
        "            # Replace current state with next state\n",
        "            ...\n",
        "\n",
        "            # If there are enough transitions in the replay memory extract a\n",
        "            # minibatch from the memory and call the optimize function to train\n",
        "            # the model on the minibatch\n",
        "            if len(self._rm) > batch_size:\n",
        "                # Extract a minibatch from the replay memory (cf. self._rm.sample)\n",
        "                ...\n",
        "\n",
        "                # Call optimize on the extracted minibatch to train model\n",
        "                ...\n",
        "                \n",
        "                # Update epsilon\n",
        "                self._epsilon = max(epsilon_min, self._epsilon * epsilon_decay)\n",
        "        return episode_reward\n",
        "\n",
        "    @tf.function\n",
        "    def _optimize(self, state_s, state_next_s, action_s, rewards_s, opp_done_s):\n",
        "        \"\"\"\n",
        "        Cette méthode calcule la Q-loss, elle doit ensuite faire la\n",
        "        backpropagation.\n",
        "        \"\"\"\n",
        "        q_values_target = self._model(state_next_s)\n",
        "        q_values_target = rewards_s + (gamma * tf.reduce_max(q_values_target, axis=1)) * opp_done_s\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values_pred = self._model(state_s)\n",
        "            q_values_pred = tf.reduce_sum(tf.multiply(q_values_pred, action_s), axis=1)\n",
        "            loss = self._loss_function(q_values_target, q_values_pred)\n",
        "        # Backpropagation (compute gradients and apply gradients using tf functions)\n",
        "        ...\n",
        "\n",
        "    def _plot_history(self):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self._reward_hist)\n",
        "        plt.xlabel('episodes')\n",
        "        plt.ylabel('reward')\n",
        "        plt.title('Reward per episode')\n",
        "        plt.show()\n",
        "    \n",
        "    def real_episode(self, env):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            output = self._model(np.expand_dims(state, axis=0))\n",
        "            action_index = np.argmax(output.numpy())\n",
        "            state, _, done, _ = env.step(action_index)"
      ],
      "metadata": {
        "id": "-Nu3ODtoJAhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Entraînement de l'agent\n",
        "\n",
        "Si tout est correct vous devriez voir la récompense moyenne des 10 derniers épisodes d'apprentissage commencer à augmenter entre 50 et 100 épisodes."
      ],
      "metadata": {
        "id": "nF8Ow2q9AF74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a fully-connected model\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(units=24, input_dim=obs_size, activation='relu'),\n",
        "                             tf.keras.layers.Dense(units=24, activation='relu'),\n",
        "                             tf.keras.layers.Dense(units=nb_actions)])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss_function = tf.keras.losses.MeanSquaredError()\n",
        "replay_memory = ReplayMemory(replay_memory_capacity)\n",
        "\n",
        "# Create the agent\n",
        "agent = Agent(replay_memory, model, optimizer, loss_function, env)"
      ],
      "metadata": {
        "id": "FG8DLUTpD_Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "agent.train()"
      ],
      "metadata": {
        "id": "pIM9Vzc0D4cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize test episode\n",
        "env = gym.make('CartPole-v1')\n",
        "env = Recorder(env, './episode')\n",
        "agent.real_episode(env)\n",
        "env.play()"
      ],
      "metadata": {
        "id": "rkYVqPF3D4i2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TP_RL_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}